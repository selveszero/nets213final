# Components:

## Finding Generative Models

Difficulty: 2

Browse online for music generation models and find their code. Try to run their model with the training data and fix potential problems. Run each model to generate enough samples for the next step.

## Quality Control/Task Design
 
Difficulty: 4

Data: the HITs take in music clips generated by the models

Quality check: programatically add an audio pronunciation of a number to the end of each music clip which the Turker will have to enter into a textbox in the HIT

Design: One music clip per HIT, Turker selects a rating from 1 to 5 of how good and coherant the clip sounds and then does the quality check. The slider for playing audio will not be draggable so that the Turker cannot skip the clip without listening to it. We will then collect information on the worker for the later analysis such as age, gender and the state they are located in.

Workers: Limiting workers to the US since we will be using Western music in our data samples, 10 workers will listen to each audio clip

## Creating the HIT

Difficulty: 5

Most of the difficulty of the HIT is in the quality control aspect, the remaining components are simple data collection (1-5 ratings, additional info parameters about the Turker and their music preference, etc.). See mockup for visual example.

## Aggregating data from the HIT

Difficulty: 2

Once we have collected data from all of our HITs, there are a number of parameters across which we will be aggregating the result data. The main one will be ranking the various generated music based on the average ratings of the clips and selecting the best audio for the next iteration. After each iteration we extend the music by attaching the best clip to the end of teh current audio. Our metric for the best clip is the clip with the highest mean rating and if a tie occurs we pick the clip with the highest rating and lowest variance.

Additionally, we will aggregate based on other secondary information we ask from the Turkers (music taste, experience with music, how often they listen to music, age/location, etc) for the analysis part.

## Analysis

Difficulty: 4

Drawing conclusions based on the data aggregation on he distribution of the quality of music clips and  determine the distribution of worker preference based on the secondary information we gathered. We will create and rate 30 songs produced by the model that are of the same length as the final iterative model and were createwd in one-go. Finally we will compare iteratively generated music with one-go generated music.

## How to Use Noteworthy
Students must accept the HIT on the Amazon MTurk Worker Sandbox using the link: https://workersandbox.mturk.com/projects/3TCGLRK80L0CG7BRUKYSENZ2INWZQX/tasks?ref=w_pl_prvw.

This video provides instructions for completing the HIT: https://vimeo.com/manage/videos/542425298

Noteworthy asks students to listen to two music recordings and rate if these recordings should be concatenated. 

Step 1: Please read the instructions at the top of the task

Step 2: Listen to the original music selection, the primer, and the new recording that is appended to the primer by clicking "play". 

Step 3: At the end of the music clip, there will be a short clip that states "Please enter XX into the textbox". Please enter the number, XX, into the textbox below the recording. 

Step 4: After listening to this recording, rate how cohesive the new recording and the primer are by selection one of the five options: bad, poor, fair, good, and excellent.

Step 5: Please fill out the questions regarding your age, gender, and home state. 

If you have any questions, please email: nwthyinfo@gmail.com. 

Thanks for using Noteworthy!

## Raw Data
We use the Magenta Performance RNN model, which uses deep learning algorithms to generate songs. This model uses TensorFlow and is open source. We are using a pretrained model, performance_with_dynamics.mag, in which the music files are trained from the Lahk MINI Dataset (LMD) and have been matched using the Million Song Dataset (MSD). Furthermore, we are using a subset of the data that contain only piano rolls, have at most one time signature change events, have a 4/4 time signature, and start at time zero. This dataset has 21,425 multitrack pianorolls with no duplicates. The song are in midi format.

## Quality Control: 
In the notebook, 213_prototype_performanceRNN, we instantiate some variables such as the primer length, the iteration number, the length of the concatenated recording, the number of tracks per iteration round, and the output directories. The generated music, in the form of a list of midi files, is in the directory, "model_out/iterX", where iterX denotes the iteration round. These files are inputs to the QC module. Each midi file will be converted into audio MP3 and a short audio auto-generated by text-to-speech from the text "Please enter XX into the textbox", where XX is a random 2 digit number. The combined final audio MP3 files will be stored in a directory, "QC_out/mp3_out/iterX". A CSV file, "filepaths_and_numbers_iterX.csv" will also be created at the end which contains the two columns: audio_url (the filepaths to the MP3 files) and number (the corresponding 2 digit number within each audio). The actual MP3 files will be uploaded to Github and the HIT will take in only the filepaths and the 2-digit numbers from the "filepaths_and_numbers_iterX.csv" file, play each audio from the url filepath, and check for a match between the textbox input and the provided 2-digit number. 

We will create another HIT that contains 30 two-minute songs. 

We will use 10 workers to rate each song for both HITs and take the average as the final rating for the song. In the first HIT, we will have 8 iterations, in which we will take the recordings with the best ratings and append them to the primer. This will serve as the input for the next iteration. Thus the primer will grow by 15 seconds for each new iteration. By the final round the song will be (primer_length + 8*15) seconds and we will output the highest-rated song. 

## Aggregation: 
Because the evaluation of music is a subjective matter, we simply take the average rating for each song as its final rating.

The code for this component can be found in src/aggregation.ipynb. 

## Docs

Documents for the flow diagram and the mock up can be found at the following: docs/flow-diagram.pdf, docs/sample_design.png.

## How to run the code
The code can be found under src. 

To run the code, first save the magenta-master.zip and 213_prototype_performanceRNN.ipynb from src and upload to google drive. 
Follow the instructions in 213_prototype_performanceRNN.ipynb to generate music from a primer. Also open up the directory magenta-master/magenta/models/performance_rnn in google drive. The primers and the pretrained model are located there. 

The directory magenta-master/magenta/models/performance_rnn/model_out will contain the output generated music (primer + model generated continuation) as midi files. 

The directory magenta-master/magenta/models/performance_rnn/QC_out/mp3_out will contain the final audio outputs (output music + validation message) as mp3 files. Manually upload these files to this github repo, under data/mp3_files/iterX, where X is the iteration number of the current iteration.

The directory magenta-master/magenta/models/performance_rnn/QC_out/ will also contain a csv file called 'filepaths_and_numbers_iterX.csv". This is the file to be passed into the HIT. 

After running the HITs and aggregation, we will get the best music of this iteration, which will be used as the primer for the next iteration. Continue extending the best music iteratively until getting a music of the desired length. 

## Analysis

For analysis, we will compare this iterative method with 2 other methods for extending the primer using the same generative model, one with the help of the crowd but no iteration, the other without the help of the crowd. 

Say if we want the final generated music to be 2 minutes long, with a 10s primer and 110s of generated music. 

The iterative method will generate, for example, 15 samples containing 10s after the primer per iteration for 11 iterations, and in each iteration the crowd will select the best sample out of the 15 to be the primer for the next iteration. 

The method with the help of the crowd but no iteration will generate, 15 samples containing 110s after the primer. The crowd will select the best sample out of the 15. 

The last method with no help of the crowd will generate 1 sample containing 110s after the primer directly.

Then, we will use the crowd for a final round of comparison where they rate the best sample of each method, and we expect the iterative method to perform better than the others.

We will also analyze the distribution of self-reported age/gender/location and their effects on worker's rating. 

