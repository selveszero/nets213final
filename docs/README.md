# Components:

## Finding Generative Models

Difficulty: 1

Browse online for music generation models and find their code. Try to run their model with the training data and fix potential problems. Run each model to generate enough samples for the next step.

## Quality Control/Task Design
 
Difficulty: 4

Data: the HITs take in music clips generated by the models

Quality check: programatically add an audio pronunciation of a number to the end of each music clip which the Turker will have to enter into a textbox in the HIT

Design: One music clip per HIT, Turker selects a rating from 1 to 5 of how good and coherant the clip sounds and then does the quality check. The slider for playing audio will not be draggable so that the Turker cannot skip the clip without listening to it. We will then collect information on the worker for the later analysis such as age, gender and the state they are located in.

Workers: Limiting workers to the US since we will be using Western music in our data samples, 10 workers will listen to each audio clip

## Creating the HIT

Difficulty: 1

Most of the difficulty of the HIT is in the quality control aspect, the remaining components are simple data collection (1-5 ratings, additional info parameters about the Turker and their music preference, etc.). See mockup for visual example.

## Aggregating data from the HIT

Difficulty: 2

Once we have collected data from all of our HITs, there are a number of parameters across which we will be aggregating the result data. The main one will be ranking the various generated music based on the average ratings of the clips and selecting the best audio for the next iteration. After each iteration we extend the music by attaching the best clip to the end of teh current audio. Our metric for the best clip is the clip with the highest mean rating and if a tie occurs we pick the clip with the highest rating and lowest variance.

Additionally, we will aggregate based on other secondary information we ask from the Turkers (music taste, experience with music, how often they listen to music, age/location, etc) for the analysis part.

## Analysis

Difficulty: 3

Drawing conclusions based on the data aggregation on he distribution of the quality of music clips and  determine the distribution of worker preference based on the secondary information we gathered. We will create and rate 30 songs produced by the model that are of the same length as the final iterative model and were createwd in one-go. Finally we will compare iteratively generated music with one-go generated music.

## How to Use Noteworthy
Students must accept the HIT on the Amazon MTurk Worker Sandbox using the link: https://workersandbox.mturk.com/projects/3TCGLRK80L0CG7BRUKYSENZ2INWZQX/tasks?ref=w_pl_prvw.

Noteworthy asks students to listen to two music recordings and rate if these recordings should be concatenated. 

Step 1: Please read the instructions at the top of the task

Step 2: Listen to the original music selection, the primer, and the new recording that is appended to the primer by clicking "play". 

Step 3: At the end of the music clip, there will be a short clip that states "Please enter XX into the textbox". Please enter the number, XX, into the textbox below the recording. 

Step 4: After listening to this recording, rate how cohesive the new recording and the primer are by selection one of the five options: bad, poor, fair, good, and excellent.

Step 5: Please fill out the questions regarding your age, gender, and home state. 

If you have any questions, please email: nwthyinfo@gmail.com. 

Thanks for using Noteworthy!

## Raw Data
We use the Magenta Performance RNN model, which uses deep learning algorithms to generate songs. This model uses TensorFlow and is open source. We are using a pretrained model, performance_with_dynamics.mag, in which the music files are trained from the Lahk MINI Dataset (LMD) and have been matched using the Million Song Dataset (MSD). Furthermore, we are using a subset of the data that contain only piano rolls, have at most one time signature change events, have a 4/4 time signature, and start at time zero. This dataset has 21,425 multitrack pianorolls with no duplicates. The song are in midi format.

## Quality Control: 
In the notebook, 213_prototype_performanceRNN, we instantiate some variables such as the primer length, the iteration number, the length of the concatenated recording, the number of tracks per iteration round, and the output directories. The generated music, in the form of a list of midi files, is in the directory, "model_out/iterX", where iterX denotes the iteration round. These files are inputs to the QC module. Each midi file will be converted into audio MP3 and a short audio auto-generated by text-to-speech from the text "Please enter XX into the textbox", where XX is a random 2 digit number. The combined final audio MP3 files will be stored in a directory, "QC_out/mp3_out/iterX". A CSV file, "filepaths_and_numbers_iterX.csv" will also be created at the end which contains the two columns: audio_url (the filepaths to the MP3 files) and number (the corresponding 2 digit number within each audio). The actual MP3 files will be uploaded to Github and the HIT will take in only the filepaths and the 2-digit numbers from the "filepaths_and_numbers_iterX.csv" file, play each audio from the url filepath, and check for a match between the textbox input and the provided 2-digit number. 

We will create another HIT that contains 30 two-minute songs. 

We will use 10 workers to rate each song for both HITs and take the average as the final rating for the song. In the first HIT, we will have 8 iterations, in which we will take the recordings with the best ratings and append them to the primer. This will serve as the input for the next iteration. Thus, the primer, will grow by 15 seconds for each new iteration. By the final round the song will be (primer_length + 8*15) seconds and we will output the highest-rated song. 

## Aggregation: 
Because the evaluation of music is a subjective matter, we simply take the average rating for each song as its final rating.

The code for this component can be found in the simple_aggregation.ipynb. 

## Analysis
We will segment our demographic data after each iteration to see how the population changes from the beginning to the end of this process. Additionally, we will compare the highest rated songs in the iterative process to those in the non-iterative HIT. 

Documents for the flow diagram and the mock up can be found at the following: docs/flow-diagram.pdf, docs/mock_up.png.

